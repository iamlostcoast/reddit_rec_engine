{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Subreddit  Recommendation System\n",
    "\n",
    "Currently reddit doesn't have an explicit \"recommendation system\" for subreddits, just the upvote system.  What if we could build a recommendation system for subreddits, that popped up in a personalized feed, or sent recs to your inbox?\n",
    "\n",
    "First we're going to need to get data on users and the articles they've upvoted.  Given that Reddit has hundreds of millions of users, we're probably going to want to start by doing some pre-clustering or just focus on followers of a particular subreddit to make this problem smaller.  From there we could potentially scale up, or write a script that will build this feature out for other subreddits.\n",
    "\n",
    "We can start by using the reddit API.  Let's try doing followers of the Data Science Reddit, and only articles posted on their.  This will probably already be a pretty large data set.\n",
    "\n",
    "### Developing a user score for a subreddit\n",
    "\n",
    "We're going to need to develop a user preference score for every subreddit.  This could use:\n",
    "- upvotes\n",
    "- downvotes\n",
    "- gilds\n",
    "- views?\n",
    "\n",
    "It could be interested to do this:\n",
    "\n",
    "(upvotes - downvotes) / views + (weight * gilds)\n",
    "\n",
    "Unforunately all we can get is comments, so we'll have to just go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import global_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it seems like we need to use oauth so we don't get a 403 response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'access_token': u'7stdgah4w7zUdf8nSsybJQgQW8E',\n",
       " u'expires_in': 3600,\n",
       " u'scope': u'*',\n",
       " u'token_type': u'bearer'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests.auth\n",
    "client_auth = requests.auth.HTTPBasicAuth('i0viuENaWrt_NA', 'AEjbSfKYCg1AK-mxF-dh7N6lp6c')\n",
    "post_data = {\"grant_type\": \"password\", \"username\": \"iamlostcoast\", \"password\": global_vars.get_reddit_pass()}\n",
    "headers = {\"User-Agent\": \"bot 0.1\"}\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yay! We got a json for the upvoted articles for a particular user.  This is a start\n",
    "\n",
    "headers = {\"Authorization\": \"bearer 7stdgah4w7zUdf8nSsybJQgQW8E\", \"User-Agent\": \"bot 0.1\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/user/iamlostcoast/upvoted.json\", headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!\n",
    "\n",
    "Now let's see if we can get a count of upvotes by subreddit of the posts on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = response.json()\n",
    "\n",
    "user_sub_ups = {}\n",
    "\n",
    "# we can scroll through meta data on upvoted articles in the data/children elements of the json file\n",
    "for element in data['data']['children']:\n",
    "    subreddit = element['data']['subreddit_name_prefixed']\n",
    "    if subreddit in user_sub_ups:\n",
    "        user_sub_ups[subreddit] += 1\n",
    "    else:\n",
    "        user_sub_ups[subreddit] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'r/EarthPorn': 1,\n",
       " u'r/GetMotivated': 1,\n",
       " u'r/aww': 10,\n",
       " u'r/funny': 2,\n",
       " u'r/gaming': 1,\n",
       " u'r/gifs': 2,\n",
       " u'r/pics': 4,\n",
       " u'r/todayilearned': 1,\n",
       " u'r/videos': 2,\n",
       " u'r/worldnews': 1}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sub_ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting User Comments per Subreddit\n",
    "\n",
    "Because we can't actually get user upvotes, we'll have to settle for user comments on particular subreddits as a measure of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to be able to get this info for a bunch of different users, preferably those who follow r/datascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try using praw to get a bunch of user comments on random subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "r = praw.Reddit(client_id = 'i0viuENaWrt_NA', client_secret='AEjbSfKYCg1AK-mxF-dh7N6lp6c', user_agent = \"bot 0.1\",\n",
    "               password=global_vars.get_reddit_pass(), username='iamlostcoast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's just make a long list of authors.\n",
    "\n",
    "# we'll start off with 200000\n",
    "authors = []\n",
    "for (i, comment) in enumerate(r.subreddit('all').stream.comments()):\n",
    "    if i <= 200000:\n",
    "        authors.append(comment.author)\n",
    "        # Going to check in every 10000\n",
    "        if i % 10000 == 0:\n",
    "            print i\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data collection pipeline\n",
    "\n",
    "Now that we've confirmed that we can pretty easily collect the data and start to use it to build a rec engine, I want to set up a little data pipeline that will collect comment data every 24 hours and save it to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first let's turn the author collection process into a function\n",
    "import praw\n",
    "\n",
    "def get_authors(count=10000, check_on=10000):\n",
    "    authors = []\n",
    "    \n",
    "    r = praw.Reddit(client_id = 'i0viuENaWrt_NA', client_secret='AEjbSfKYCg1AK-mxF-dh7N6lp6c', user_agent = \"bot 0.1\",\n",
    "               password=global_vars.get_reddit_pass(), username='iamlostcoast')\n",
    "    \n",
    "    for (i, comment) in enumerate(r.subreddit('all').stream.comments()):\n",
    "        if i <= count:\n",
    "            authors.append(comment.author)\n",
    "            # Going to check in every 10000\n",
    "            if i % check_on == 0:\n",
    "                print i\n",
    "        else:\n",
    "            break\n",
    "    return authors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-to-item collaborative Filtering\n",
    "\n",
    "\n",
    "This is probably the type of rec engine we want to do, because it's easy to add in new users.  We don't need to find the interactions between the new user and every other user, we just need to find the subs most similar to the ones they like.\n",
    "\n",
    "**Item-Item Collaborative filtering:** It is quite similar to previous algorithm, but instead of finding customer look alike, we try finding item look alike. Once we have item look alike matrix, we can easily recommend alike items to customer who have purchased any item from the store. This algorithm is far less resource consuming than user-user collaborative filtering. Hence, for a new customer the algorithm takes far lesser time than user-user collaborate as we donâ€™t need all similarity scores between customers. And with fixed number of products, product-product look alike matrix is fixed over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.Connection(\"./reddit_rec_data.sqlite\")\n",
    "data = pd.read_sql(\"SELECT * FROM comment_data\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DCbean</td>\n",
       "      <td>r/10cloverfieldlane</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fakedeepusername</td>\n",
       "      <td>r/1200isplenty</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Shyguy380</td>\n",
       "      <td>r/13ReasonsWhy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>jamjax12</td>\n",
       "      <td>r/13ReasonsWhy</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Death215</td>\n",
       "      <td>r/2007scape</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              user             variable  value\n",
       "0      0            DCbean  r/10cloverfieldlane    1.0\n",
       "1      1  fakedeepusername       r/1200isplenty    4.0\n",
       "2      2         Shyguy380       r/13ReasonsWhy    1.0\n",
       "3      3          jamjax12       r/13ReasonsWhy    5.0\n",
       "4      4          Death215          r/2007scape    2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the most popular subreddits are.\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.rename(columns={'variable': 'subreddit', 'value': 'comments'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop = data[['subreddit', 'comments']].groupby(\"subreddit\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r/AskReddit</th>\n",
       "      <td>146488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/politics</th>\n",
       "      <td>53627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/The_Donald</th>\n",
       "      <td>35898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/nba</th>\n",
       "      <td>31910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/worldnews</th>\n",
       "      <td>18621.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/news</th>\n",
       "      <td>18371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/funny</th>\n",
       "      <td>17766.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/videos</th>\n",
       "      <td>17543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/pics</th>\n",
       "      <td>17308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r/leagueoflegends</th>\n",
       "      <td>17140.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   comments\n",
       "subreddit                  \n",
       "r/AskReddit        146488.0\n",
       "r/politics          53627.0\n",
       "r/The_Donald        35898.0\n",
       "r/nba               31910.0\n",
       "r/worldnews         18621.0\n",
       "r/news              18371.0\n",
       "r/funny             17766.0\n",
       "r/videos            17543.0\n",
       "r/pics              17308.0\n",
       "r/leagueoflegends   17140.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10 = pop.sort_values('comments', ascending=False).head(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11752e450>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAGoCAYAAAB2RTwLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8pnO9//HXmhkaUyOnxURNSnyUCDkMezBEmEZSElE5\n02aXDr8UKmzHtiQlx0K7HCJSszcqZshxOlBTzEfFNjrQYGQyEmb9/riuae7GOs3MWt973fd6PR+P\nedz3/b2+13V/ru+yxnu+16mjq6sLSZIkqaQRzS5AkiRJw48hVJIkScUZQiVJklScIVSSJEnFGUIl\nSZJU3KhmF6Al88ILL3bNnTu/2WUMaSuvPAbHqGeOT98co945Pn1zjPrmGPWuXcans3NsR0/LnAlt\nMaNGjWx2CUOeY9Q7x6dvjlHvHJ++OUZ9c4x6NxzGxxAqSZKk4gyhkiRJKq7DJya1ljnnfssfmCRJ\nWjZ77l7kazwnVJIkSUOKIVSSJEnFGUIlSZJUnCFUkiRJxRlCJUmSVJwhVJIkScUZQiVJklScIVSS\nJEnFGUIlSZJUnCFUkiRJxRlCJUmSVJwhVJIkScWNanYBgyUirgQOyMz5EbEXcDGwbmb+qZd1jgce\nzczzGtr2B04EHqybVgJuz8wj+lnHacCszLxksfZHM3NcRHwauBn4FbBfZl7Uz12UJElqWW05ExoR\no4ERmTm/bjoEOBs4dCk3eVlmTsrMScCmwMYRsdmyVwqZeVpmzgDGAQcPxDYlSZKGuraZCa1nLA+k\nCtanAtPq9tcBqwCnAz+PiJMz8/mIeDdwNPA88Cdg74ZtvQG4jO5D4Viq2dC/RsRywHnAuvX3HpeZ\n0yPiPcBxwBxgeWBWRIwELgA2AH4PvKz+rkuAK4D3AG+KiM9l5okDNCySJElDUrvNhM7NzInAZGBq\n3XYQ8I3MfAq4E3h33b4P8F91/6nAinV7UAXQfTPzV3Xb+yPiloh4ALgJODkzf0sVUh/PzG2B3YFz\n6mB6JrAjsDOwcDZ2D2B0Zk4APgOMWaz2k4H7DKCSJGk4aLcQmvXr+MycXc8+7gfsGRE3AOsBR9Z9\nPg7sEBG3AFsDC+r2XakC4osN270sM7ejCpVjgQfq9g2ByRExHfgu1czyq4AnM/OJzOwC7qj7rgfM\nAMjM2cAjA7bXkiRJLabdQuiCiNgImFl/ngz8NDO3z8xdMnMLYI26z6HA8XW47KCaqQQ4C/gYcGkd\nYv8pMx8CjgCuiogxwCzg8vpc0V2Bq4BHgZUiorNebfP69T5gK4CIWBNYa/Haab+fhyRJUrfaMfRM\nYdGh+EOA/15s+UVUs6EzgKkRcRPVRUEL1yEzf0QVGo9efOOZ+WPgx8AJwPnA+vVs6h3Aw5n5j3r7\nN0bEj6nOCQW4DngiIu6mCrqPL7bpvwDLR8TpS7PTkiRJraSjq6ur2TVoCcw591v+wCRJ0rLZc/ci\nX9PZObajp2XtOBMqSZKkIc4QKkmSpOIMoZIkSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhkiRJKs4Q\nKkmSpOIMoZIkSSrOJya1nq45c+Y1u4YhrbNzLI5RzxyfvjlGvXN8+uYY9c0x6l27jI9PTJIkSdKQ\nYgiVJElScYZQSZIkFWcIlSRJUnGGUEmSJBVnCJUkSVJxhlBJkiQVN6rZBWjJzDpn92aXMOQ90ewC\nhrh2GZ9V9/pWs0uQJC0DZ0IlSZJUnCFUkiRJxRlCJUmSVJwhVJIkScUZQiVJklScIVSSJEnFGUIl\nSZJUnCFUkiRJxRlCJUmSVJwhVJIkScUZQiVJklScIVSSJEnFGUK7ERFXRsSYJei/dkTcVb+/IiKW\nj4jxEbFb3XZWRIwfrHolSZJazahmFzDURMRoYERmzl+a9TNz73o7OwDrAz/IzKMGsERJkqSWZwgF\nImJ/4ECqmeFTgWkRMQk4FlgAjAMuyMxzImIT4CvAi8DfgUMW29b/ARsAnwbGRMQdwMeBw4EngEuB\nlYAO4IPA6sAXgeeB+cCemTlv0HZWkiRpCPBw/CJzM3MiMBmYWretBbwTmAB8LCJWBy4EjszM7YCv\nAWd2s60XgdOAyzLz+w3txwHfz8ytgU8AWwDvAr4DbAecC6w80DsmSZI01BhCF8n6dXxmzq7f35GZ\nz2Xms8CvgXWANTPz3nr5rVSznv0VwJ0AmXlHZn4bOAVYE7gJ2JNqRlSSJKmtGUIXWRARGwEzG9o2\njoiR9UVKGwC/Bf5U94Nq9vKBnrbHS8f3fmBzgIjYNiJOB/YDLsnM7YHfAIcOyN5IkiQNYZ4T+q+m\nsOhQPMBywPXAqsBJmfl4RBwCfDUiOoAXgIN62NZM4NiI+EVD2ynANyJiP6CrXrcTuCginqEKroZQ\nSZLU9gyhQGZe0sOi+xde7d7Q9x5g2276TqiXr11/vofq8DvAFQ39dltsvQcXritJkjRceDhekiRJ\nxTkT2oPMnA5Mb3IZkiRJbcmZUEmSJBVnCJUkSVJxhlBJkiQVZwiVJElScYZQSZIkFWcIlSRJUnGG\nUEmSJBVnCJUkSVJx3qy+xax/xHXMmTOv2WUMaZ2dYx2jXjg+kqShwJlQSZIkFWcIlSRJUnGGUEmS\nJBVnCJUkSVJxhlBJkiQVZwiVJElScd6iqcVcffEuzS5BQ8B2U65qdgmSJC0TZ0IlSZJUnCFUkiRJ\nxRlCJUmSVJwhVJIkScUZQiVJklScIVSSJEnFGUIlSZJUnCFUkiRJxRlCJUmSVJwhVJIkScUZQiVJ\nklScIVSSJEnFGUIlSZJU3KhmF7A0IuJK4D5ge2AlYM36M8BBwLczc8IybH9/4ETgQaqg3gWckJk3\nL0PZjds/Hng0M89brP3RzBw3EN8hSZI0lLXcTGhEjAZGZOYJmTkJOAq4OTMn1Z9fHKCvuqze5rbA\nXsC5EWFAlCRJGgAtMRNaz0weSBWaTwWm9bFKZ0R8D3gV8KvMPCQiXgNcAKwAPAscmpmP9Of7M/Ox\niPguMCUiLgUuBl4PjATOzMwrI2I6cC/wZmBF4L2Z+XBEnApsBqwK/DIzD2jYr5F1TRsAvwde1p96\nJEmSWl0rzYTOzcyJwGRgah99VwQOALYC3hYRqwNnAGfXs6VnAKct4fc/BqwGHAbMycytgR2BkyJi\ntbrPjMzcEfgRsE9ErFjXvRNVEJ0QEWs1bHMPYHR96sBngDFLWJMkSVJLaqUQmvXr+Myc3UffBzNz\nbmYuAP5CFe42BI6pZyw/B6yxhN//WuAPwBuBWwEycx7Vuajr1H3uqV8fAUZTzbiuHhGXA+cDrwCW\na9jmesCMeluz6/UkSZLaXiuF0AURsREwsx99u7ppmwUcXc+EHgZc1d8vjohXAbsD/wvcD2xTt4+l\nCrcP9fC9uwKvycx9gGOoTgXoaFh+H9VsLRGxJrAWkiRJw0BLnBPaYAp9H4rvySepLi4aTRUGP9pH\n//dHxASqC506gAMy88mIuAC4MCJuq7dzQmb+JSK628YM4LMRcStVQH2Q6kr+ha4DdoqIu4GHgceX\nct8kSZJaSkdXV3eThhqqrr54F39gYrsp/Z7If4nOzrHMmTNvAKtpP45R7xyfvjlGfXOMetcu49PZ\nObajp2WtNhM6oCLiGmCVxZr/mpm7N6MeSZKk4WJYh9DMfHeza5AkSRqOWunCJEmSJLUJQ6gkSZKK\nM4RKkiSpOEOoJEmSijOESpIkqThDqCRJkoozhEqSJKm4YX2f0Fa05wE3tMUTFAZTuzxlQpKkduZM\nqCRJkoozhEqSJKk4Q6gkSZKKM4RKkiSpOEOoJEmSijOESpIkqThv0dRijv/Ozs0uQQPoiO2vbnYJ\nkiQ1hTOhkiRJKs4QKkmSpOIMoZIkSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOIM\noZIkSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhAygiroyIMT0suyQidildkyRJ0lBkCB0gETEaGJGZ\n85tdiyRJ0lA3qtkFtLKI2B84kCrMnwpMi4hJwNHAP4DXA1dk5sn1Kv8eEf+PatwPyszfRcSpwGbA\nqsAvM/OAsnshSZJUnjOhy25uZk4EJgNT67bXAu8BJgCfauh7R2a+DTgd+EJErFivvxNVEJ0QEWuV\nK12SJKk5DKHLLuvX8Zk5u34/MzNfyMxngGcb+t5av94BRL1s9Yi4HDgfeAWwXIGaJUmSmsoQuuwW\nRMRGwMyGtq4e+m5Rv24D/BrYFXhNZu4DHAOsAHQMVqGSJElDheeEDowpLDoU35sJEXEzVUg9EHgO\n+GxE3Fq3PQisCTw0WIVKkiQNBYbQZZCZl3TTNh2Y3vB5XP26fw+b2XzgK5MkSRraPBwvSZKk4gyh\nkiRJKs4QKkmSpOIMoZIkSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOIMoZIkSSrO\nx3a2mOP3upE5c+Y1u4whrbNzrGMkSdIQ50yoJEmSijOESpIkqThDqCRJkoozhEqSJKk4Q6gkSZKK\nM4RKkiSpOG/R1GImX3tSs0sYUi6d+NFmlyBJkpaCM6GSJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOIM\noZIkSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOIMoZIkSSrOECpJkqTi2i6ERsSV\nETFmALbzaDdth0fE8cu6bUmSpOGuzxAaERt00zZhcMpZNhExGhiRmfObXYskSZJ6NqqnBRHxb8BI\n4KKIOAjoaFjnPGC9wS+vbxGxP3AgVaA+FZgWEdcCJ2fmzyJiFnBMZl4TET8EDgAmAUcBzwG/BQ4F\n9m3Yzucbtj8R+DIwF3gBuCsi1gYuBx4B1gFmZOaHI+KVwNeBVevVPwLsACyXmWdExHnAPzLzIxFx\nLPAQsBLwIWAB8NPM/MjAj5IkSdLQ0ttM6E7ACcCrgBPr9ycAnwHOH/zSlsjczJwITAamAtcCu0bE\n66iC5o51QBwN/J1qP3ao13kKOKxxO5l5U8O2zwX2ycwdqULjQusBBwFbAJMjYhxwDHBTZm5PFWzP\nrWvZpV4ngC3r97vUtR4AHJmZWwH3R0SP/zCQJElqFz0Gnsw8HiAiPpCZ/12soqWT9ev4zJwdET8A\nrgMeB04HPg7sCvwAeD3wm8ycV69zK/B24O6G7TRaIzMfqN/fDryhfv+7hduIiD9TBdwNgR0i4n11\nn1XqesZExBbA/cD4iNgc+GtmPh0RBwCfrAPznSyacZYkSWpbPc6ENlyAs0NEfGPxP2XK67cFEbER\nMBMgM+cC84H3ATcAs4GPAtdQzWa+KSJeXq+7HbAwZC7oZtt/jIg31u83b2jv6qbvLOBLmTkJ2Av4\nVt3+P8AXgB/Wf75CNUMKcAhweGZuB2wCbN2/XZYkSWpdvR2O/3n9Oh24pZs/Q80UqsPbC10HjMnM\nJ4Eb6/e/z8zHqc75nBYRdwGrUR0278lhwDcj4ibgtX3UcDKwV0RMpwq/v67brwH+Dbi5rmWzuj6o\ngvNPIuJm4C9UM7KSJEltraOrq7sJPYiI8b2tmJmzB6Ui9WrytSd1/wMbpi6d+NGXtHV2jmXOnHnd\n9BY4Pv3hGPXO8embY9Q3x6h37TI+nZ1jezzNsLeLYG6hOuQ8GlgDeBB4keqcyN8B6w9gjZIkSRpG\nejwcn5mvy8zXU124Mykz183M9YGtqM+9lCRJkpZGf56Y9MbM/MnCD5n5U5wFlSRJ0jLozz0p/xAR\nJwJXUoXW/Vh0NbkkSZK0xPozE7ofsDJwBfBtYDlg/0GsSZIkSW2uz5nQ+p6b/1GgFkmSJA0TvT07\nfgH/ekP256lu5v4y4OnMXHmQa5MkSVKb6u3q+BGZORK4APgQsEJmjqF6EtDVheqTJElSG+rPOaFb\nZua3MrMLIDO/y78+vlKSJElaIv25Ov6ZiDgA+A5VaP0A8MSgViVJkqS21p8Quh/wVeBsqnNCf0wV\nRNUE/7vHcW3xGC9JkjS89efq+IeB3SJilcx8skBNkiRJanN9htCI2JjqHqFjImIC1WM898rMXwx2\ncZIkSWpP/bkw6WxgD+CJzPwT8GHgvEGtSpIkSW2tPyF0TGbev/BDZv6I6l6hkiRJ0lLpTwh9MiLe\nQn3j+ojYF/DcUEmSJC21/lwd/2HgUmCDiHgK+C2w76BWJUmSpLbWnxC6U2ZOjIiXAyMz8+nBLko9\nm3L1t5tdwpBx8XbvbHYJkiRpKfUnhB4JnJeZzwx2MZIkSRoe+hNCH4mIm4G7gWcXNmbmiYNWlSRJ\nktpaf0LoXQ3vOwarEEmSJA0f/Xli0gkRsRywPvA88NvMfHHQK5MkSVLb6vMWTRGxLfB74GLgMmBW\nRGw22IVJkiSpffXncPyXgHdk5kyAOoB+DdhiMAuTJElS++rPzepZGEDr9z+jf+FVkiRJ6laPYbI+\nDA/V4ffzgK8DL1DdqH5GgdokSZLUpnqb0Txhsc9faHjfNQi1SJIkaZjoMYRm5vYlC5EkSdLw0ee5\nnRExjW5mPjNzh0GpSJIkSW2vPxcYHd/wfjlgd2DuoFQjSZKkYaE/N6u/ZbGmH0fE3cDnBqekoS0i\nrgQOyMz5za5FkiSpVfXncPz4ho8dwJuBVQetoiEsIkYDIwygkiRJy6Y/h+NvoTontANYADwOHDmY\nRQ0lEbE/cCDVPVVPBaZFxCTgaOAfwOuBKzLz5Ih4DXABsALwLHAo8HHg9sy8OiJuAH6YmWdGxIVU\nT6F6B7A91c/iu5l5esn9kyRJaob+3Kx+b+AcqmfH/x5YB3j5YBY1BM3NzInAZGBq3fZa4D3ABOBT\nddsZwNmZOal+fxpwLbBrRKwArAy8LSI6gLcCd1Ldd/X9wDbAU0X2RpIkqcn6E0K/DPwUeDcwH9gE\n+PRgFjUEZf06PjNn1+9nZuYLmfkM1awnwIbAMRExneqc2TWA24BNqWY7vwt0UgXOOzOziyqEngbc\nCKxUYF8kSZKarj8hdERm3gpMoTpc/AjD77GdCyJiI2BmQ1t3N+yfBRxdz4QeBlyVmQuAn1HNlv6Q\nKpR+AbgmIl4GvBfYhyqk7h8Rrx20vZAkSRoi+hNC50fEJ4AdgKkR8VFg3uCWNSRNYdGh+J58Evh8\nRNwCfBP4Vd1+DfBG4JdUM55vAG7JzOeAJ4G7gGlUIXX24huVJElqN/2Z0dwXOAh4T2bOjYg1qc5h\nHBYy85Ju2qYD0xs+j6tfHwR27qb/9VSH5qEKoas1LDsROHEAS5YkSRry+nOf0D/SEJIy8+hBrUiS\nJEltrz+H4yVJkqQBZQiVJElScYZQSZIkFWcIlSRJUnGGUEmSJBVnCJUkSVJxhlBJkiQVZwiVJElS\ncYZQSZIkFdefx3ZqCJm6577MmTOv2WVIkiQtE2dCJUmSVJwhVJIkScUZQiVJklScIVSSJEnFGUIl\nSZJUnCFUkiRJxXmLphazx3dva3YJTXfBtm9pdgmSJGkZORMqSZKk4gyhkiRJKs4QKkmSpOIMoZIk\nSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOIMoZIkSSrOECpJkqTiDKGSJEkqblSz\nC2glEXElcADwdeANwAcyc1Zzq5IkSWo9htB+iojRwIjMnB8RO2ZmZ7NrkiRJalWG0F5ExP7AgVSn\nLZwKTIuIrwGvjIjrgGuB9TPz03VInZWZa0fEdOBe4M3AisB7gQ7gcuARYB1gRmZ+OCJuBw7NzN9E\nxK7Abpn570V3VJIkqTDPCe3b3MycCEwGptYB8cnM3L2P9WZk5o7Aj4B96rb1gIOALYDJETEOuAj4\nUL38wPqzJElSWzOE9i3r1/GZObuXfh2Lfb6nfn0EGF2//11mzsvMF4E/1+3fAd4ZEasDr87MXwxQ\n3ZIkSUOWIbRvCyJiI2BmN8v+Dryqfr/pYsu6uun/krbMfAaYBnwZ+NYy1ClJktQyDKH9MwWY2k37\nDcDaEXEbsBfw9FJu/0Jgd+DbS7m+JElSS/HCpF5k5iU9tI+rX58Ctutm+aSG9+c1LJrQ0D6hoX0k\ncHW9PUmSpLZnCG2yiDiS6mKlvZpdiyRJUimG0CbLzK8CX212HZIkSSV5TqgkSZKKM4RKkiSpOEOo\nJEmSijOESpIkqThDqCRJkoozhEqSJKk4Q6gkSZKKM4RKkiSpOG9W32Kufc9E5syZ1+wyJEmSlokz\noZIkSSrOECpJkqTiDKGSJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOK8RVOLOefax5pdQlPtNXFMs0uQ\nJEkDwJlQSZIkFWcIlSRJUnGGUEmSJBVnCJUkSVJxhlBJkiQVZwiVJElScYZQSZIkFWcIlSRJUnGG\nUEmSJBVnCJUkSVJxhlBJkiQVZwiVJElScYbQXkTElRExppv2jSPic920XxERk4oUJ0mS1MJGNbuA\noSoiRgMjMnP+4ssy817g3vJVSZIktQdDaIOI2B84kGqG+FRgWkScCfwyMy+NiHHA/wCfAA7PzL0j\n4gjgYODPwOr1dpYDzgPWrbd1XGZOj4idgJOAvwNP1N+1HHBl3W90vV0DriRJamsejn+puZk5EZgM\nTAUuAj5UL/sAcPHCjhGxBvBRYAKwO7B8vehg4PHM3LZuPyciOoALgHdn5nbALcBxwBZUgXRX4Ajg\n5YO6d5IkSUOAM6EvlfXr+MycDRARoyLitcD7gB2Bjes+6wC/yczn6n4z6vYNgW0iYsv68yhgNeDp\nzPxj3XYrcArwKaoZ0+uA56lmSiVJktqaM6EvtSAiNgJmNrR9HfgCcF9mPtXQ/ltgg4hYISJGApvU\n7bOAyzNzEtUM51XAk8CKEfGqus92wAPAJODPmfl2qgB6yqDslSRJ0hBiCO3eFKpD8QtdBexMdWj+\nnzJzDnAacAdwPfBMveh8YP2IuKVe9nBmvggcAlwTEbdTzaj+J/BL4OCImA78F9W5qJIkSW2to6ur\nq9k1aAmcc+1jw/oHttfEl9wx6yU6O8cyZ868AtW0Jsenb45R7xyfvjlGfXOMetcu49PZObajp2XO\nhEqSJKk4Q6gkSZKKM4RKkiSpOEOoJEmSijOESpIkqThDqCRJkoozhEqSJKk4Q6gkSZKKM4RKkiSp\nOEOoJEmSihvV7AK0ZI7YY422eIyXJEka3pwJlSRJUnGGUEmSJBVnCJUkSVJxhlBJkiQVZwiVJElS\ncYZQSZIkFectmlrMPRf9pdklNM2rd1+h2SVIkqQB4kyoJEmSijOESpIkqThDqCRJkoozhEqSJKk4\nQ6gkSZKKM4RKkiSpOEOoJEmSijOESpIkqThDqCRJkoozhEqSJKk4Q6gkSZKKM4RKkiSpOEPoAImI\nKyNiTA/LzoqI8aVrkiRJGqpGNbuAdhARo4ERmTm/u+WZeVThkiRJkoY0Q+hSioj9gQOpZpNPBaZF\nxCTgWGABMA64IDPPiYjpwOHAE8ClwEpAB/BBYHXgi8DzwHxgz8ycV3JfJEmSSvNw/LKZm5kTgcnA\n1LptLeCdwATgYxGxekP/44DvZ+bWwCeALYB3Ad8BtgPOBVYuVLskSVLTGEKXTdav4zNzdv3+jsx8\nLjOfBX4NrNPQP4A7ATLzjsz8NnAKsCZwE7An1YyoJElSWzOELpsFEbERMLOhbeOIGFlfpLQB8NuG\nZfcDmwNExLYRcTqwH3BJZm4P/AY4tEzpkiRJzeM5octuCosOxQMsB1wPrAqclJmPR8TCZacA34iI\n/YAu4CCgE7goIp6hOpfUECpJktqeIXQpZeYlPSy6PzP3XqzvpIaPuy3W/0Gq80clSZKGDQ/HS5Ik\nqThnQgdQZk4Hpje5DEmSpCHPmVBJkiQVZwiVJElScYZQSZIkFWcIlSRJUnGGUEmSJBVnCJUkSVJx\nhlBJkiQVZwiVJElScd6svsVscvDqzJkzr9llSJIkLRNnQiVJklScIVSSJEnFGUIlSZJUnCFUkiRJ\nxRlCJUmSVJwhVJIkScUZQiVJklSc9wltMY+d9fNml9AUI/Zdr9klSJKkAeRMqCRJkoozhEqSJKk4\nQ6gkSZKKM4RKkiSpOEOoJEmSijOESpIkqThDqCRJkoozhEqSJKk4Q6gkSZKKM4RKkiSpOEOoJEmS\niiv27PiIuBK4FPhgZu5d6nt7ExErAzcBTwB/BK7IzBsKffddwN6Z+X8lvk+SJGkoKTITGhGj6++a\nX+L7lsCGwEOZuVOzC5EkSRpOBm0mNCL2Bw6kCp+nAtMWW/5e4OPAi8BtmfnpiHg1cC4wGngVcFxm\nfi8ipgAnAn8F5gK/AqYDhy+cVY2IRzNzXES8BrgAWAF4Fjg0Mx+JiE8AewMvALcCnwXOBtaMiBMa\n6loOOA9Yt679uMyc3l0NmXl8RJwKbAOMBM7MzKsiYjpwL/BmYEXgvZn5cEScDOwCPAKsVn/fvwFf\nBJ6nCul7Zua8pR95SZKkoW+wZ0LnZuZEYDIwdWFjRKwCnAC8rV6+VkTsBKwPfLGemTwUOCIiRlKF\nxV0zc3uqYNmbM4CzM3NS/f60iNgQ2AvYuv6zLvB24Cjg5sz8fMP6BwOPZ+a2wO7AOT3VEBG7Aq+r\n92F74NiIWKnezozM3BH4EbBPRGwGbAtsDnwQGFv3exfwHWA7qgC+cl+DKkmS1OoG+5zQrF/HZ+bs\niHh9/fkNQCfwvxEBVSBbB/gJcFxEHAR0AcvV/Z7OzMfqdX8CjOvmuzrq1w2BYyLi6Lrteapwe1dm\nPg8QET8BNgDu7mY7GwLbRMSW9edRVLOy3dWwIfDWeuaTut616/f31K+P1H3XA36WmQuApyNiZr38\nFOBYqnNT/9hDTZIkSW1lsGdCF0TERsDMxdofogpnO9Uzll8B7gL+E/hmZn6A6vB9B/AXYGxEdNbr\nTqhf/04VDomI1wKr1O2zgKPr7R4GXFW3bRkRoyKig2pG8oEeap4FXF6vv2u9/p96qGEWMK3uuwPV\njObv62Vdi233PmCLiBgRES8H3lS37wdcUs+w/oZqBliSJKmtlbgwaQoNh+IBMnMOcCZwS0TcTRX2\nHqAKfGdExK3ATsBq9czhkVSzpj8GxlPNbv4MeKpe/wSqYAvwSeDzEXEL8E2qczdnUgXE24EZwP8B\n3+uh3vOPbB9AAAAJoElEQVSB9ev17wAe7qWGHwB/q2dWfw509XQ+Z2beC1wP/BS4gipcU9dzUUTc\nRBVkv9nLWEqSJLWFjq6uxSfshp6I+AzVRT/PRcS3gB9mZtGwNhRqAHjsrJ8P/R/YIBix73r97tvZ\nOZY5c7y2qyeOT98co945Pn1zjPrmGPWuXcans3NsR0/Lit0ndBnNA+6KiPlUs5hXDtMaJEmS2kJL\nhNDM/Crw1eFegyRJUrvwsZ2SJEkqzhAqSZKk4gyhkiRJKs4QKkmSpOIMoZIkSSrOECpJkqTiDKGS\nJEkqzhAqSZKk4lriZvVaZI2j3toWj/GSJEnDmzOhkiRJKs4QKkmSpOIMoZIkSSquo6urq9k1SJIk\naZhxJlSSJEnFGUIlSZJUnCFUkiRJxRlCJUmSVJwhVJIkScUZQiVJklScIVSSJEnF+ez4FhERI4Cv\nAW8BngMOzszfNbeqwRURywHfANYGXgacBNwHXAJ0Ab8GjsjMBRHxeeAdwAvAUZk5IyLe0N++Jfdr\nMETE6sDPgZ2o9usSHCMAIuIzwDuB5al+h27B8fmn+vfsUqrfsxeBQ/C/oX+KiC2B0zNz0pLs60D0\nLbmfS2ux8dkY+ArVf0fPAR/MzMci4hDgMKp9Pikzp0bEasBlwArAn4ADMnP+kvQtvKtLrXGMGtre\nD/xHZm5Vfx6WY+RMaOt4FzC6/g/208AXm1xPCfsBT2TmNsAuwFeBM4Hj6rYOYPeI2BTYDtgS2Bs4\np15/Sfq2rDpEnA88Wzc5RrWImARsDfwb1T69BsdncZOBUZm5NXAicDKOEQAR8SngImB03TRY4/KS\nvoO9bwOhm/H5MlWwmgRcAxwdEeOAj1D9Du4MnBoRLwM+B1xW7/M9wGFL0rfQLi6zbsaIiNgEOIjq\nZ81wHiNDaOuYCNwAkJl3AZs1t5wirgI+W7/voPpX31upZrIArgd2pBqbH2ZmV2bOBkZFROcS9m1l\nZwDnUf3rFxyjRjsDM4FrgR8AU3F8FvcA1T6MAFYEnscxWuj3wLsbPg/WuHTXtxUsPj57Z+a99ftR\nwN+BLYDbM/O5zPwr8DtgIxr+n8aifV6Svq3iX8YoIlYFTgGOaugzbMfIENo6VgT+2vD5xYho69Mp\nMvNvmTkvIsYCVwPHAR2ZufBZs/OAV/LSsVnYviR9W1JE7A/MycwbG5odo0VWo/oH23uBw4FvAyMc\nn3/xN6pD8bOAC4Gz8b8hADLzu1ShfKHBGpfu+g55i49PZv4ZICK2Bo4EvkTP+9zY3tf4dNe3JTSO\nUUSMBL4OfJxqPxYatmNkCG0dTwNjGz6PyMwXmlVMKRHxGmAa8N+ZeRnQeJ7UWOApXjo2C9uXpG+r\nOhDYKSKmAxsD3wRWb1g+3MfoCeDGzPxHZibVzEzjX87DfXwAPkY1RutRnXN+KdX5sws5RosM1t8/\n3fVtSRHxPqojM+/IzDn0vM+N7X2NT3d9W9FbgXWBc4ErgDdFxFkM4zEyhLaO26nO3SIiJlAdYmxr\nEbEG8EPg6Mz8Rt18T32eH8CuwE+oxmbniBgREeOpAvrjS9i3JWXmtpm5XX0O1r3AB4HrHaN/ug3Y\nJSI6ImJN4OXATY7Pv5jLohmUJ4Hl8PesJ4M1Lt31bTkRsR/VDOikzHywbp4BbBMRoyPilcAbqS6+\n+uf/01i0z0vSt+Vk5ozM3KD++3pv4L7MPIphPEZtfTi3zVxLNeN1B9X5kQc0uZ4SjgFWBj4bEQvP\nDf0ocHZELA/cD1ydmS9GxE+AO6n+YXVE3fcTwIX97NtOlmS/23qM6qtGt6X6i3vhvjyE49PoS8A3\n6n1anur37mc4Rt0ZrN+tl/QttkcDpD7UfDYwG7gmIgBuyczPR8TZVKFoBHBsZv49Ik4CLo3qSu/H\ngfdn5jP97Vt8BwdRZj46XMeoo6urq+9ekiRJ0gDycLwkSZKKM4RKkiSpOEOoJEmSijOESpIkqThD\nqCRJkoozhEpSG4mITSPi9GbXsSQiYreI+Hgvy18REdfUtwGS1CYMoZLUXr4EtFQIpXqSzIo9LczM\nvwE/Bg4rVpGkQed9QiVpENVPwjmW6iET61DdiPyvwLvqtsnAJsCJVE8regg4JDOfiIj3Ut3IfIX6\nz8GZeWv9mNYZwDZAJ/AfmXl9ROwAHJqZe9ff/X7gOKAL+ClwSP0dF1I9onMBcEZmfjMi9gfeAawF\nvBo4CxgP7ED1+NNdgXHA94AHgQ2pbmo/Hdif6sESe2Tm/RGxOVUYHkN14+zDMvOh7uoGHgZurofr\nM1Q3O/9CXfNcYJ/MfDwiVgHuAqLhOeuSWpgzoZI0+LakesrZBsCHgTmZuRnwK+Bw4DRg58zcBLgR\nOD0iRtTLpmTmW+o+/69hm8tn5lZUz34/qW57J3ArQESsRRUE356ZGwAjqULm8cATmflmqoB5fERs\nVK+/BbALVUj8InB9Zi5ctnP9uhHwn0AAmwNr13VcDhxaP/XnIqqnu2xab+fCnurOzPuonjV+XmZe\nTBWaD6/H5wfApgCZ+STwt/r7JbUBH9spSYPv15n5CEBEPA7cVLc/DOxGNeM4rX7U4UjgycxcEBF7\nALtFtWAS8GLDNm9YuG1glfr9uiyaVdwKuD0z/wCQmR+ov/844KC67fGIuK7e9tN1/6eBp+taGutc\nuX7/aGbeU2/rD4v1eR2wHtWM7/frbcC/Hmrvru5G3weujYjvAddl5o8alj1c7+Mvu1lPUotxJlSS\nBt8/Fvv8QsP7kcBtmblxZm5MNbu4Z0S8guoQ+uuoZjfPpjp8v9Df69euhvYFDdt+vvELI6IzIjp5\n6d/7HSyakPiXOjPzBV6qt31ZuD8PNuzPW4GJfdTd+J1fogrFvwO+EBHHNix+nmofJbUBQ6gkNdfd\nwFYRsV79+bPAf1HNKC4ATqGa3dyVKuD15vfAa+v3PwW2jIhx9ecvAbvX2zoIICJWozo3dfpA7Eht\nFrBKRGxTfz4QuKyPdV6gDsIRcTcwNjPPqmvetKHf66jCqaQ24OF4SWquR6mC2nfqWxD9AdgPeAq4\nlyrUzQduYVHA7MkPqK4gPzcz/xQRHwVurLd7J3Ax8HLgaxExkyrUnpyZv2g4L3SZZOZz9QVVX46I\n0VSH+T/Ux2q3ApdGxGPAMcAlEfEC8CzVebFExErAKzPzVwNRp6Tm8+p4SWoTEdEB3AbsnpmPN7ue\ngVQH6hcy85xm1yJpYHg4XpLaRH3roqOAo5tdy0Cqz4/dETi/2bVIGjjOhEqSJKk4Z0IlSZJUnCFU\nkiRJxRlCJUmSVJwhVJIkScUZQiVJklTc/wcSEepIOVNPuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111780d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.barplot(x=top_10['comments'], y=top_10.index, orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ask reddit shits all over everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One way to do this would be to just do cosine similiarity or pearson similiarity between subreddits.\n",
    "\n",
    "# Let's try this with a sample of the data\n",
    "\n",
    "sample = data[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(sample['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DCbean</td>\n",
       "      <td>r/10cloverfieldlane</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fakedeepusername</td>\n",
       "      <td>r/1200isplenty</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shyguy380</td>\n",
       "      <td>r/13ReasonsWhy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jamjax12</td>\n",
       "      <td>r/13ReasonsWhy</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Death215</td>\n",
       "      <td>r/2007scape</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               user            subreddit  comments\n",
       "0            DCbean  r/10cloverfieldlane       1.0\n",
       "1  fakedeepusername       r/1200isplenty       4.0\n",
       "2         Shyguy380       r/13ReasonsWhy       1.0\n",
       "3          jamjax12       r/13ReasonsWhy       5.0\n",
       "4          Death215          r/2007scape       2.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse = sample.pivot(index='user', columns='subreddit', values='comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7903, 7903)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7903, 7677)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so looks like our cosine similarity matrix has calculated the cosine similarity between each user and each other user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering With Cosign Similarities\n",
    "\n",
    "Let's try building a cosign similarity matrix and serving predictions based on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we need a function to calculate the cosine similarity between two users\n",
    "\n",
    "def cosine_similarity(user_a, user_b):\n",
    "    a_dot_b = sum([a * b for a, b in zip(user_a, user_b)])\n",
    "    magnitude_a = sum([a**2 for a in user_a])**0.5\n",
    "    magnitude_b = sum([b**2 for b in user_b])**0.5\n",
    "    cosine_sim = a_dot_b/(magnitude_a * magnitude_b)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68098266919850992"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(sparse.iloc[0, :], sparse.iloc[15, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'-StormDrake-'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.index[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looks like our function is all good!\n",
    "\n",
    "# Now let's add a function for finding the most similar users to a particular user\n",
    "\n",
    "def most_similar(data, user, num_users):\n",
    "    user_vector = data.loc[user, :].values\n",
    "    similarities = []\n",
    "    for other_user in data.index:\n",
    "        other_user_vector = data.loc[other_user, :].values\n",
    "        sim = cosine_similarity(user_vector, other_user_vector)\n",
    "        similarities.append((sim, other_user))\n",
    "    top_num = sorted(similarities)[-num_users:][::-1]\n",
    "    return top_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storm_drake = most_similar(sparse.iloc[0:1000, :], '-StormDrake-', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is fucking slow as balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, u'DestilShadesk'),\n",
       " (1.0, u'Bluesky201516'),\n",
       " (1.0, u'-StormDrake-'),\n",
       " (0.0, u'DjentlemanThall3612'),\n",
       " (0.0, u'Django-Django')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_drake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Okay so this works, but it does suck ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cos_sim_df = pd.DataFrame(data=cos_sim, columns=sparse.index, index=sparse.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seems like a much faster way would be to pre build the entire matrix and then serve responses.\n",
    "\n",
    "# Let's keep going with this method first.\n",
    "\n",
    "def get_recs(data, sims, user, num_recs):\n",
    "    totals = {}\n",
    "    simSums = {}\n",
    "    ranking_list = []\n",
    "    for other_user in sims.index:\n",
    "        sim = sims.loc[user, other_user]\n",
    "        \n",
    "        # Now we have to loop through the other person's subreddits\n",
    "        for i, rating in enumerate(data.loc[other_user, :].values):\n",
    "            \n",
    "            # only rate reddits that this user has already commented on\n",
    "            if data.loc[user, :].values[i] == 0:\n",
    "                subreddit = data.columns[i]\n",
    "                # Similrity * score\n",
    "                totals.setdefault(subreddit,0)\n",
    "                totals[subreddit] += rating * sim\n",
    "                \n",
    "                # sum of similarities\n",
    "                simSums.setdefault(subreddit,0)\n",
    "                simSums[subreddit]+= sim\n",
    "                \n",
    "    rankings = [(total/simSums[item], item) for item,total in totals.items()]\n",
    "    rankings = sorted(rankings)[::-1]\n",
    "    # returns the recommended items\n",
    "    recommendataions_list = [recommend_item for score, recommend_item in rankings][:num_recs]\n",
    "    return recommendataions_list\n",
    "            \n",
    "%time get_recs(sparse, cos_sim_df, '-StormDrake-', 5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving on this Method\n",
    "\n",
    "Using this type of memory based recommendation system is clearly not going to work for this situation.  It's far too slow, and also suffers from the \"cold-start\" problem, where we can't recommend things to users who are not already in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparsity = round(1.0-sparse.shape[0]/float(sparse.shape[0]*sparse.shape[1]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99987"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.Connection(\"./reddit_rec_data.sqlite\")\n",
    "data = pd.read_sql(\"SELECT * FROM comment_data\", con=conn)\n",
    "data.rename(columns={'variable': 'subreddit', 'value': 'comments'}, inplace=True)\n",
    "\n",
    "# We'll start off by just doing this on a sample\n",
    "sample = data[:50000]\n",
    "\n",
    "sparse = sample.pivot(index='user', columns='subreddit', values='comments')\n",
    "sparse_csc = csc_matrix(sparse, dtype=np.float32)\n",
    "\n",
    "# We'll use the csc_matrix format to speed things up a bit.\n",
    "\n",
    "users = sparse.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's right a simple function that creates an empty dictionary for users we want to get predictions for\n",
    "\n",
    "def get_test_users(users, sparse):\n",
    "    # This function should get the row id for the users we want to get predictions for\n",
    "    user_index = list(sparse.index)\n",
    "    user_dict = {}\n",
    "    for user in users:\n",
    "        user_dict[user_index.index(user)] = []\n",
    "    return user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_user_dict = get_test_users(['-StormDrake-'], sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DCbean</td>\n",
       "      <td>r/10cloverfieldlane</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fakedeepusername</td>\n",
       "      <td>r/1200isplenty</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Shyguy380</td>\n",
       "      <td>r/13ReasonsWhy</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>jamjax12</td>\n",
       "      <td>r/13ReasonsWhy</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Death215</td>\n",
       "      <td>r/2007scape</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              user            subreddit  comments\n",
       "0      0            DCbean  r/10cloverfieldlane       1.0\n",
       "1      1  fakedeepusername       r/1200isplenty       4.0\n",
       "2      2         Shyguy380       r/13ReasonsWhy       1.0\n",
       "3      3          jamjax12       r/13ReasonsWhy       5.0\n",
       "4      4          Death215          r/2007scape       2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reddits_comm(user_dict, data, users):\n",
    "    user_ids = user_dict.keys()\n",
    "    reddits_comm = {}\n",
    "    for user_id in user_ids:\n",
    "        username = users[user_id]\n",
    "        data_filtered = data[data['user'] == username]\n",
    "        reddits_comm[username] = list(data_filtered['subreddit'].values)\n",
    "    return reddits_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sparsesvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import csv\n",
    "from sparsesvd import sparsesvd\n",
    "\n",
    "def computeSVD(urm, K):\n",
    "    U, s, Vt = sparsesvd(urm, K)\n",
    "\n",
    "    dim = (len(s), len(s))\n",
    "    S = np.zeros(dim, dtype=np.float32)\n",
    "    for i in range(0, len(s)):\n",
    "        S[i,i] = mt.sqrt(s[i])\n",
    "\n",
    "    U = csr_matrix(np.transpose(U), dtype=np.float32)\n",
    "    S = csr_matrix(S, dtype=np.float32)\n",
    "    Vt = csr_matrix(Vt, dtype=np.float32)\n",
    "\n",
    "    return U, S, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import * #used for matrix multiplication\n",
    "\n",
    "def computeEstimatedRatings(urm, U, S, Vt, user_dict, moviesSeen, K, test):\n",
    "    rightTerm = S*Vt \n",
    "\n",
    "    estimatedRatings = np.zeros(shape=(urm.shape[0], urm.shape[1]), dtype=np.float16)\n",
    "    for userTest in user_dict:\n",
    "        prod = U[userTest, :]*rightTerm\n",
    "\n",
    "        #we convert the vector to dense format in order to get the indices of the reddits with the most estimated comments \n",
    "        estimatedRatings[userTest, :] = prod.todense()\n",
    "        recom = (-estimatedRatings[userTest, :]).argsort()[:250]\n",
    "        for r in recom:\n",
    "            if r not in moviesSeen[userTest]:\n",
    "                uTest[userTest].append(r)\n",
    "\n",
    "                if len(uTest[userTest]) == 5:\n",
    "                    break\n",
    "\n",
    "    return uTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(sparse):\n",
    "    K = 10\n",
    "    urm = sparse\n",
    "    print \"Computing SVD\"\n",
    "    U, S, Vt = computeSVD(urm, K)\n",
    "    print \"Getting test users\"\n",
    "    uTest = get_test_users(['-StormDrake-'], sparse)\n",
    "    print \n",
    "    moviesSeen = get_reddits_comm(uTest, sample)\n",
    "    uTest = computeEstimatedRatings(urm, U, S, Vt, uTest, moviesSeen, K, True)\n",
    "    return uTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SVD\n"
     ]
    }
   ],
   "source": [
    "test = main(sparse_csc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Implementing a Simpler SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.Connection(\"./reddit_rec_data.sqlite\")\n",
    "data = pd.read_sql(\"SELECT * FROM comment_data\", con=conn)\n",
    "data.rename(columns={'variable': 'subreddit', 'value': 'comments'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse = data.pivot(index='user', columns='subreddit', values='comments')\n",
    "sparse_csc = csc_matrix(sparse, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(sparse_csc, k = 10)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 0.181279048154\n"
     ]
    }
   ],
   "source": [
    "print 'User-based CF MSE: ' + str(sqrt(mean_squared_error(X_pred, np.nan_to_num(sparse_csc.toarray()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7903, 7677)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
